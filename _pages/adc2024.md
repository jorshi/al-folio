---
layout: external
permalink: /adc2024/
nav: false
---

# TorchDrum
<p>An audio-driven 808 drum synthesiser built with PyTorch and JUCE</p>

<div style="padding-bottom:10px">
Jordie Shier<sup>1</sup>, Charalampos Saitis<sup>1</sup>, Andrew Robertson<sup>2</sup>, and Andrew McPherson<sup>3</sup>
<div style="font-size:small">
  <sup>1</sup>Centre for Digital Music, Queen Mary University of London<br/>
  <sup>2</sup>Ableton AG<br />
  <sup>3</sup>Dyson School of Design Engineering, Imperial College London<br/>
</div>
</div>

{% include figure.html path="assets/img/adc2024/plugin-ui.jpg" title="TorchDrum UI" class="img-fluid rounded z-depth-1" width="90%" %}
<div style="font-size:smaller">
TorchDrum user interface designed by Lewis Wolstanholme and Francis Devine of <a href="https://juliaset.bandcamp.com/">Julia Set.</a>
</div>

<hr />

<div align="left">
    <a href="https://github.com/jorshi/torchdrum-plugin"><i class="fab fa-github"></i> <b>Audio Plugin Code</b></a> | <a href="https://github.com/jorshi/ddsp-timbre-remap"><i class="fab fa-github"></i> <b>Training Code</b></a> | <a href="https://github.com/jorshi/torchdrum-plugin/releases/download/v0.0.6/TorchDrum-0.0.6.Mac.OSX.Silicon.Only.pkg"><i class="fas fa-headphones"></i> <b>Audio Plugin</b></a> | <a href="https://colab.research.google.com/drive/1nwV5y2eYiCF9YIM1BSmKU9uiPBbw9OxV?usp=sharing"><i class="fas fa-laptop-code"></i> <b>Google Colab</b></a> | <a href="https://arxiv.org/abs/2407.04547"><i class="fas fa-paperclip"></i> <b>Paper</b></a> |  <a href="https://youtu.be/IVaNfp4rev0"><i class="fas fa-video"></i> <b>Presentation</b></a>
</div>

<hr />

## Contents

- [What is TorchDrum?](#what-is-torchdrum)
- [Demo](#demo)
- [Timbre Remapping](#timbre-remapping)
- [Parameter Mapping Neural Network](#parameter-mapping-neural-network)
- [Creating a Plugin](#creating-a-plugin)
- [Further Reading](#further-reading)

## What is TorchDrum?

TorchDrum is an audio plugin that transforms input audio signals into a synthesised 
808 drum in real-time.
It is a synthesiser that receives audio (not MIDI).
The rhythm, loudness, and timbral variations from a percussive
input signal are mapped to synthesiser controls, allowing for dynamic, audio-based
control. The figure below shows the main signal flow implemented in the audio plug-in, which was
built in C++ using JUCE.

{% include figure.html path="assets/img/adc2024/real-time-slide.jpg" title="Real-time Plugin Oveview" class="img-fluid rounded z-depth-1" width="100%" %}

<br/>
 The main signal processing and mapping components are:
- 1) **Onset Detection**: drum hits in the input signal are detected using an onset detection
algorithm. This allows the rhythm from the input to be mapped to the synthesiser.
- 2) **Feature Extraction**: a short segment of audio is extracted from the input signal for
feature extraction. We only use 256 samples which is ~5.3ms (48kHz sampling rate), this
minimises the delay between a detected onset and triggering the synth. This segment of 256
samples is passed into an audio feature extractor that measures the loudness, spectral centroid (brightness),
and spectral flatness (noisiness) of the input.
- 3) **Parameter Mapping Neural Network**: These audio features are then passed into a
neural network that has been trained to produce synthesiser parameter modulations for the
808 synthesiser so the output follows the variations in loudness and timbre observed at
the input.

We refer to the process of automatically updating synthesiser parameters to reflect the timbral
variations in the input signal as *timbre remapping*.
<br/>
<hr />

## Demo

<iframe width="770" height="440" src="https://www.youtube.com/embed/V3ht2JhvJjA?si=ey2OYjWirZM5AGn5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<br />
<hr />

## Timbre Remapping

Timbre remapping involves the analysis and transfer of timbre from an audio input onto
controls for a synthesizer.
To achieve timbre remapping we consider musical phrases where
timbre changes from note to note. We look at the *differences in timbre* (and loudness) between
notes and try to recreate those differences on our synthesiser. The process
is similar to transposing a melody into a different key, but instead of pitch, we are dealing
with melodies of timbre and loudness, and are transposing from an input instrument onto
synthesiser.

Let's make this a bit more concrete. Here is an example of a short snare drum phrase
with plots of the timbre and loudess for each hit. The arrow shows differences between each note
compared to the first note in the phrase.
<iframe width="770" height="440" src="https://www.youtube.com/embed/X8ivQVtUQQc?si=bO6a6xFqT-2sFDlX&loop=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Here's the same snare drum phase (without the rhythm) followed by all the differences
in timbre and loudness mapped onto a synthesiser. What we end up is a new a synthesiser
sound (a modulated preset) for each hit in the input phrase.
<iframe width="770" height="440" src="https://www.youtube.com/embed/cdY3EB3O7TM?si=GGN6hkTJd0HjTJhh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<br />
<hr />

## Parameter Mapping Neural Network

The neural network enables real-time timbre remapping, allowing for input signals to be
transformed with low-latency. The neural network we use is relatively small, but it contains
non-linearities, allowing for complex relationships between input features and parameters
to be formed. 

The neural network is trained to generate synthesiser parameters
to match the loudness and timbral changes over full the full length of a drum hit (~1 second).
During real-time operation we can't wait a full second to see how the input sound
fully evolves, so we predict parameters based on short segments of audio at detected
onsets.

We also employ a recent method called *differentiable digital signal processing* (DDSP),
which enables DSP algorithms to be integrated directly into gradient-descent based optimization
used to train neural networks. Practically speaking, this allows us to compute training
error on the audio output of our synthesiser using loudness and timbral features.
We don't need to know the parameters for our timbre remapping ahead of time -- we only
need our input audio which we use during *self-supervised* training.

This figure outlines the training process:

{% include figure.html path="assets/img/adc2024/neural-net-training.jpg" title="Neural Network Training" class="img-fluid rounded z-depth-1" width="100%" %}
<br />
The main takeaway from the figure is that we're training the neural network to match
*differences* in audio features between two difference hits. $$y = f(x_a) - f(x_b)$$
is the difference between two different sounds in an input, and $$\hat{y} = f(x_c) - f(x_d)$$
is the difference between a synthesiser preset and a modulated version of that
preset. The goal of the neural network is create a parameter modulation $$\theta_{mod}$$, which is added
to a static preset $$\theta_{pre}$$ so that $$y = \hat{y}$$. 
Furthermore, the neural network is trained to make this prediction
from onset features $$f_o(\cdot)$$ to allow for real-time parameter prediction.

<br />
<hr />

## Creating a Plugin

Neural network training is conducted in Python using [PyTorch](https://pytorch.org/). After
training, the neural network is exported to [torchscript](https://pytorch.org/docs/stable/jit.html),
allowing it to be loaded into a [JUCE Plugin](https://juce.com/) written in C++ with
[torchlib](https://pytorch.org/cppdocs/).

The components required for real-time inference, namely the onset detection, onset features,
and the synthesiser were rewritten in C++ for the audio plug-in. To make sure the Python
and C++ matched, we wrote unit tests that loaded the C++ into Python using [cppyy](https://cppyy.readthedocs.io/en/latest/)
and compare the outputs.

Here's a visual overview of the components required for training in Python and what 
was included in the real-time plug-in.

{% include figure.html path="assets/img/adc2024/implementation-details.gif" title="Implementation Details" class="img-fluid rounded z-depth-1" width="100%" %}

<br />
<hr />

## Further Reading

If you're interested in learning more deeply about this research and it's background,
we first recommend checking out the [conference video presentation](https://www.youtube.com/watch?v=IVaNfp4rev0&ab_channel=JordieShier) 
and the [paper](https://arxiv.org/abs/2407.04547), which was published at the 2024 
New Interfaces for Musical Expression conference. You can find a more detailed description
of the methods and all the background references.

If you're more of a code person then check out this [Google Colab](https://colab.research.google.com/drive/1nwV5y2eYiCF9YIM1BSmKU9uiPBbw9OxV?usp=sharing)
which provides a tutorial on training new mapping models. Repositories containing [training code](https://github.com/jorshi/ddsp-timbre-remap)
and the [audio plugin](https://github.com/jorshi/torchdrum-plugin) are also available.

Below is a brief curated list of papers and resources that were most influential in
the design of this research.

**Differentiable Digital Signal Processing**

- Engel, Jesse, et al. "[DDSP: Differentiable digital signal processing.](https://arxiv.org/abs/2001.04643)" arXiv preprint arXiv:2001.04643 (2020).

- Hayes, Ben, et al. "[A review of differentiable digital signal processing for music and speech synthesis.](https://doi.org/10.3389/frsip.2023.1284100)" Frontiers in Signal Processing 3 (2024): 1284100.

- Hayes, Ben, et al. "[Introduction to DDSP for Audio Synthesis.](https://intro2ddsp.github.io/intro.html)" ISMIR Tutorial (2023).

**Timbre Space, Timbre Analogies, and Timbre Remapping**

- Grey, John M. "[Multidimensional perceptual scaling of musical timbres.](https://doi.org/10.1121/1.381428)" the Journal of the Acoustical Society of America 61.5 (1977): 1270-1277.

- Wessel, David L. "[Timbre space as a musical control structure.](https://www.jstor.org/stable/3680283)" Computer music journal (1979): 45-52.

- Stowell, Dan, and Mark D. Plumbley. "[Timbre remapping through a regression-tree technique.](https://scholar.google.com/scholar_url?url=https://www.academia.edu/download/41516099/7.pdf&hl=en&sa=T&oi=gsb-ggp&ct=res&cd=1&d=11065372655903924951&ei=eqgkZ_npOdmDy9YP0Zu88QU&scisig=AFWwaebLvcShov4eQL73pI1Shi2a)" Sound and Music Computing (SMC) (2010).
