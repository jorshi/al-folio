---
Articles
---

@article{turian2022hear,
  abbr={arXiv},
  abstract={What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without fine-tuning? The aim of the HEAR benchmark is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR evaluates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. HEAR was launched as a NeurIPS 2021 shared challenge. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.},
  title={Hear 2021: Holistic evaluation of audio representations},
  author={Turian, Joseph and Shier, Jordie and Khan, Humair Raj and Raj, Bhiksha and Schuller, Bj{\"o}rn W and Steinmetz, Christian J and Malloy, Colin and Tzanetakis, George and Velarde, Gissel and McNally, Kirk and Henry, Max and Pinto, Nicolas and Noufi, Camille and Clough, Christian and Herremans, Dorien and Fonseca, Eduardo and Engel, Jesse and Salamon, Justin and Esling, Philippe and Manocha, Pranay and Watanabe, Shinji and Jin, Zeyu and Bisk, Yonatan},
  journal={arXiv preprint},
  year={2022},
  arxiv={2203.03022}
}

@inproceedings{turian2021one,
  abbr={DAFx},
  abstract={We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.},
  title={One billion audio sounds from gpu-enabled modular synthesis},
  author={Turian, Joseph and Shier, Jordie and Tzanetakis, George and McNally, Kirk and Henry, Max},
  equal={Turian, Joseph and Shier, Jordie},
  booktitle={2021 24th International Conference on Digital Audio Effects (DAFx)},
  pages={222--229},
  year={2021},
  organization={IEEE},
  pdf={https://dafx2020.mdw.ac.at/proceedings/papers/DAFx20in21_paper_34.pdf},
  video={https://dafx2020.mdw.ac.at/proceedings/presentations/DAFx20in21_paper_34.mp4},
  code={https://github.com/torchsynth/torchsynth},
  website={https://torchsynth.readthedocs.io/en/latest/}
}

@article{shier2021manifold,
  abbr={JAES},
  abstract={The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The task of organizing and selecting from these large collections can be challenging and time consuming, which points to the need for improved methods for user interaction. This paper presents a system that computationally characterizes and organizes drum machine samples in two dimensions based on sound similarity. The goal of the work is to support the development of intuitive drum sample browsing systems. The methodology presented explores time segmentation, which isolates temporal subsets from the input signal prior to audio feature extraction, as a technique for improving similarity calculations. Manifold learning techniques are compared and evaluated for dimensionality reduction tasks, and used to organize and visualize audio collections in two dimensions. This methodology is evaluated using a combination of objective and subjective methods including audio classification tasks and a user listening study. Finally, we present an open-source audio plug-in developed using the JUCE software framework that incorporates the findings from this study into an application that can be used in the context of a music production environment.},
  title={Manifold learning methods for visualization and browsing of drum machine samples},
  author={Shier, Jordie and McNally, Kirk and Tzanetakis, George and Brooks, Ky Grace},
  journal={Journal of the Audio Engineering Society},
  volume={69},
  number={1/2},
  pages={40--53},
  year={2021},
  pdf={shier2021manifold_greenoa.pdf},
  publisher={Audio Engineering Society},
  link1_url={https://github.com/jorshi/sample_analysis},
  link1_text={Analysis Code},
  link2_url={https://github.com/jorshi/sieve},
  link2_text={Audio Plugin}
}

@inproceedings{shier2020spiegelib,
  abbr={AES},
  abstract={Automatic synthesizer programming is the field of research focused on using algorithmic techniques to generate parameter settings and patch connections for a sound synthesizer. In this paper, we present the Synthesizer Programming with Intelligent Exploration, Generation, and Evaluation Library (spiegelib), an open-source, object oriented software library to support continued development, collaboration, and reproducibility within this field. spiegelib is designed to be extensible, providing an API with classes for conducting automatic synthesizer programming research. The name spiegelib was chosen to pay homage to Laurie Spiegel, an early pioneer in electronic music. In this paper we review the algorithms currently implemented in spiegelib, and provide an example case to illustrate an application of spiegelib in automatic synthesizer programming research.},
  title={Spiegelib: An automatic synthesizer programming library},
  author={Shier, Jordie and Tzanetakis, George and McNally, Kirk},
  booktitle={Audio Engineering Society Convention 148},
  year={2020},
  organization={Audio Engineering Society},
  pdf={https://www.aes.org/e-lib/online/browse.cfm?elib=20794},
  poster={shier2020spiegelib_poster.pdf},
  code={https://github.com/spiegelib/spiegelib},
  website={https://spiegelib.github.io/spiegelib/}
}

@inproceedings{shier2017analysis,
  abbr={AES},
  abstract={The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The development of new tools to assist users organizing and managing libraries of this type requires comprehensive audio analysis that is distinct from that used for general classification or onset detection tasks. In this paper 4230 kick and snare samples, representing 250 individual electronic drum machines are evaluated. Samples are segmented into different lengths and analyzed using comprehensive audio feature analysis. Audio classification is used to evaluate and compare the effect of this time segmentation and establish the overall effectiveness of the selected feature set. Results demonstrate that there is improvement in classification scores when using time segmentation as a pre-processing step.},
  title={Analysis of drum machine kick and snare sounds},
  author={Shier, Jordie and McNally, Kirk and Tzanetakis, George},
  booktitle={Audio Engineering Society Convention 143},
  year={2017},
  organization={Audio Engineering Society},
  pdf={https://www.aes.org/e-lib/browse.cfm?elib=19284},
  poster={shier2017analysis_poster.pdf}
}

@inproceedings{shier2017sieve,
  abbr={WIMP},
  abstract={The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. To be efficient, users of these large collections require new tools to assist them in sorting, selection and auditioning tasks. This paper presents a new plugin for working with a large collection of kick and snare samples within a music production context. A database of 4230 kick and snare samples, representing 250 individual electronic drum machines are analyzed by segmenting the audio samples into different sample lengths and characterizing these segments using audio feature analysis. The resulting multidimensional feature space is reduced using principle component analysis (PCA). Samples are mapped to a 2D grid interface within an audio plug-in built using the JUCE software framework.},
  title={Sieve: A plugin for the automatic classification and intelligent browsing of kick and snare samples},
  author={Shier, Jordie and McNally, Kirk and Tzanetakis, George},
  booktitle={3rd Workshop on Intelligent Music Production. WIMP},
  year={2017},
  pdf={shier2017sieve_paper.pdf},
  poster={shier2017sieve_poster.pdf},
  code={https://github.com/jorshi/sieve}
}